{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll create some rule-based classifiers. In this workbook, we'll practice working with Python and the FNC data, and create a simple FNC predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anytime you see ``______ # TODO: FILL IN HERE.`` in the code, you should replace the ``______`` with your own code.\n",
    "\n",
    "As always, ask your neighbors or an instructor if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import the packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have ``train_data.csv`` and ``test_data.csv`` saved, run the following code to construct and save csvs of the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_data(stances_filename, bodies_filename, merged_filename):\n",
    "    stances = pd.read_csv(stances_filename, encoding = \"utf-8\")\n",
    "    bodies = pd.read_csv(bodies_filename, encoding = \"utf-8\")\n",
    "    data = pd.merge(bodies, stances, on='Body ID')\n",
    "    data.to_csv(merged_filename, index=False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where are the train_stances.csv and train_bodies.csv files?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are inside the AI4ALL_NLP_student folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_data(\"../train_stances.csv\", \"../train_bodies.csv\", \"../train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_data(\"../competition_test_stances.csv\", \"../competition_test_bodies.csv\", \"../test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we add a \"../\" in front of the names of our .csv files?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_stances.csv, train_bodies.csv, competition_test_stances.csv, and competition_test_bodies.csv are saved in the folder one level above the folder we're currently in. We also want to save the merged train_data.csv and test_data.csv in the folder one level above the folder we're currently in. This will make it easier to access these files in future days, since we work in a separate folder (inside the AI4ALL_NLP_student folder) each day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to find all six .csv files in Finder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Separate the dataset based on stance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in and run the following cells to separate the train and test sets based on their stance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../train_data.csv\", encoding = \"utf-8\")\n",
    "unrelated_train = train_data[train_data['Stance'] == 'unrelated']\n",
    "discuss_train = train_data[train_data['Stance'] == 'discuss']\n",
    "agree_train = train_data[train_data['Stance'] == 'agree']\n",
    "disagree_train = train_data[train_data['Stance'] == 'disagree']\n",
    "\n",
    "test_data = pd.read_csv(\"../test_data.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a classifer based on shared words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first classifier will be based on the percentage of headline words that appear in the article. We'll find the average percentage for each stance. When our classifier gets a headline-article pair, it'll find the percentage of headline words in the article and then predict the stance with the closest average percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, try to write out the steps of this classifier, and the components we'll need to write to create the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Write a function that finds the proportion of headline words in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_headline_in_article_proportion(example):\n",
    "    headline_words = str.split(example['Headline'])\n",
    "    article_words = str.split(example['articleBody'])\n",
    "    counter = 0\n",
    "    for word in headline_words:\n",
    "        if word in article_words:\n",
    "            counter += 1\n",
    "    proportion = counter/len(headline_words)\n",
    "    return proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain in plain English what this function does:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the proportion of overlapping words between the headline and article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out this function. Before running the following code, try to predict what the output will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_text = \"this is a test headline\"\n",
    "article_text = \"this is a test article\"\n",
    "test_df = pd.DataFrame({'Headline':[headline_text], 'articleBody':[article_text]})\n",
    "example = test_df.iloc[0]\n",
    "find_headline_in_article_proportion(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test it out on a headline and article pair of your choosing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_text = \"test is this a\"\n",
    "article_text = \"this is a test\"\n",
    "test_df = pd.DataFrame({'Headline':[headline_text], 'articleBody':[article_text]})\n",
    "example = test_df.iloc[0]\n",
    "find_headline_in_article_proportion(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test out the function on an example from the FNC data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = train_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body ID                                                        0\n",
      "articleBody    A small meteorite crashed into a wooded area i...\n",
      "Headline       Soldier shot, Parliament locked down after gun...\n",
      "Stance                                                 unrelated\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we wanted to use something other than the `print` function, how could we have seen what `example` contains?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Body ID                                                        0\n",
       "articleBody    A small meteorite crashed into a wooded area i...\n",
       "Headline       Soldier shot, Parliament locked down after gun...\n",
       "Stance                                                 unrelated\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_headline_in_article_proportion(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Find the average proportion for each stance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will find the average proportion overlap between article and body for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_proportions(unrelated, discuss, agree, disagree):\n",
    "    proportions_unrelated = []\n",
    "    for i in range(unrelated.shape[0]):\n",
    "        this_example = unrelated.iloc[i]\n",
    "        proportions_unrelated.append(find_headline_in_article_proportion(this_example))\n",
    "    proportions_related = []\n",
    "    proportions_discuss= []\n",
    "    for i in range(discuss.shape[0]):\n",
    "        this_example = discuss.iloc[i]\n",
    "        proportions_discuss.append(find_headline_in_article_proportion(this_example))\n",
    "        proportions_related.append(find_headline_in_article_proportion(this_example))\n",
    "    proportions_agree= []\n",
    "    for i in range(agree.shape[0]):\n",
    "        this_example = agree.iloc[i]\n",
    "        proportions_agree.append(find_headline_in_article_proportion(this_example))\n",
    "        proportions_related.append(find_headline_in_article_proportion(this_example))\n",
    "    proportions_disagree= []\n",
    "    for i in range(disagree.shape[0]):\n",
    "        this_example = disagree.iloc[i]\n",
    "        proportions_disagree.append(find_headline_in_article_proportion(this_example))\n",
    "        proportions_related.append(find_headline_in_article_proportion(this_example))\n",
    "    return {\"unrelated\":np.mean(proportions_unrelated), \"discuss\":np.mean(proportions_discuss), \"agree\":np.mean(proportions_agree), \"disagree\":np.mean(proportions_disagree), \"related\":np.mean(proportions_related)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proportions = compute_proportions(unrelated_train, discuss_train, agree_train, disagree_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at `proportions`, try to guess what the proportions are for each category.\n",
    "Side note: We saved proportions in a **dictionary**. If you're interested in learning more about it, ask an instructor or see this description: https://www.tutorialspoint.com/python/python_dictionary.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now look at the proportions for each category.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agree': 0.44605676586182841,\n",
       " 'disagree': 0.43070433968733673,\n",
       " 'discuss': 0.43875075101834521,\n",
       " 'related': 0.44024866842925497,\n",
       " 'unrelated': 0.14348271330437723}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Write a prediction function based on the closest average proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll write a function that predicts an example's class depending on which group's overlap proportion it's closest to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(example, proportions):\n",
    "    proportions_stances = list(proportions.keys())\n",
    "    proportion = find_headline_in_article_proportion(example)\n",
    "    predicted_stance = proportions_stances[np.argmin(np.abs(np.array(list(proportions.values())) - proportion))]\n",
    "    return predicted_stance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a.  Write a function that runs and evaluates predictions on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function classifies the percentage of correct predictions for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_predictions(test_data, proportions):\n",
    "    stance_counts = {\"unrelated\":0, \"discuss\":0, \"agree\":0, \"disagree\":0}\n",
    "    stance_correct_counts = {\"unrelated\":0, \"discuss\":0, \"agree\":0, \"disagree\":0}\n",
    "    for i in range(test_data.shape[0]):\n",
    "        example = test_data.iloc[i]\n",
    "        predicted_stance = make_prediction(example, proportions)\n",
    "        actual_stance = example['Stance']\n",
    "        stance_counts[actual_stance] += 1\n",
    "        if predicted_stance == actual_stance:\n",
    "            stance_correct_counts[actual_stance] += 1\n",
    "    return {\"unrelated\":stance_correct_counts[\"unrelated\"]/stance_counts[\"unrelated\"], \"discuss\":stance_correct_counts[\"discuss\"]/stance_counts[\"discuss\"], \"agree\":stance_correct_counts[\"agree\"]/stance_counts[\"agree\"], \"disagree\":stance_correct_counts[\"disagree\"]/stance_counts[\"disagree\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Test the classifier and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_percentages = test_predictions(test_data, proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agree': 0.4703100367840252,\n",
       " 'disagree': 0.21377331420373027,\n",
       " 'discuss': 0.0035842293906810036,\n",
       " 'unrelated': 0.9000490489944956}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What categories does the predictor do well on, and what does the predictor do less well on? Why do you think this is? (hint: take a look at the average proportion of in-article headline words).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does very well on 'unrelated' and worst on 'discuss'. The mean proportion for 'unrelated' is very different from the other categories, while the 'discuss' proportion is sandwiched between the proportions for 'agree' and 'disagree'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the number of examples in each category, and compute the overall classification accuracy. Is this higher or lower than you would have expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrelated_test = test_data[test_data['Stance'] == 'unrelated']\n",
    "discuss_test = test_data[test_data['Stance'] == 'discuss']\n",
    "agree_test = test_data[test_data['Stance'] == 'agree']\n",
    "disagree_test = test_data[test_data['Stance'] == 'disagree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_accuracy = (correct_percentages['agree'] * agree_test.shape[0] \\\n",
    "                    + correct_percentages['discuss'] * discuss_test.shape[0] \\\n",
    "                    + correct_percentages['disagree'] * disagree_test.shape[0] \\\n",
    "                    + correct_percentages['unrelated'] * unrelated_test.shape[0]) \\\n",
    "                    / test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.691575178058474"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you think of this accuracy? How much better do you think we can do?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a simple classifer based on the most common category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5a. Find the most common category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find what the most common category is, using the training data. For something like this, there's often a function someone else used that we can borrow. The easiest way to find this is by googling -- for example, I googled \"pandas series count values for each type\" and this was the second result: https://stackoverflow.com/questions/22391433/count-the-frequency-that-a-value-occurs-in-a-dataframe-column. Check out this link, and try to use one line to find the number of occurrences of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stance</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agree</th>\n",
       "      <td>3678</td>\n",
       "      <td>3678</td>\n",
       "      <td>3678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disagree</th>\n",
       "      <td>840</td>\n",
       "      <td>840</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discuss</th>\n",
       "      <td>8909</td>\n",
       "      <td>8909</td>\n",
       "      <td>8909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unrelated</th>\n",
       "      <td>36545</td>\n",
       "      <td>36545</td>\n",
       "      <td>36545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Body ID  articleBody  Headline\n",
       "Stance                                   \n",
       "agree         3678         3678      3678\n",
       "disagree       840          840       840\n",
       "discuss       8909         8909      8909\n",
       "unrelated    36545        36545     36545"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('Stance').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the most common category?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common category is 'unrelated'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extra Challenge: Try determining the number of examples for each category without using these functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unrelated examples is 36545\n",
      "Number of agree examples is 3678\n",
      "Number of discuss examples is 8909\n",
      "Number of disagree examples is 840\n"
     ]
    }
   ],
   "source": [
    "stances = ['unrelated', 'agree', 'discuss', 'disagree']\n",
    "for stance in stances:\n",
    "    print(\"Number of \" + stance + \" examples is \" + str(train_data[train_data['Stance'] == stance].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b. Compute the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find how many examples of each category there are in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stance</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agree</th>\n",
       "      <td>1903</td>\n",
       "      <td>1903</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disagree</th>\n",
       "      <td>697</td>\n",
       "      <td>697</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discuss</th>\n",
       "      <td>4464</td>\n",
       "      <td>4464</td>\n",
       "      <td>4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unrelated</th>\n",
       "      <td>18349</td>\n",
       "      <td>18349</td>\n",
       "      <td>18349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Body ID  articleBody  Headline\n",
       "Stance                                   \n",
       "agree         1903         1903      1903\n",
       "disagree       697          697       697\n",
       "discuss       4464         4464      4464\n",
       "unrelated    18349        18349     18349"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.groupby('Stance').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, what would the accuracy be if we always guessed the most common category?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7220320308503522"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18349/test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Extra challenge\" sections are a more unguided exploration into the concepts we've discussed. You'll notice less scaffolding for the code -- try implementing these concepts from scratch, and feel free to ask your neighbors or an instructor if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1. Two-way classification.\n",
    "\n",
    "Try adapting our code for four-way classification (between 'unrelated', 'agree', 'disagree', and 'discuss') for two-way classification (between 'unrelated' and 'related'). To do this, we'll group 'agree', 'disagree', and 'discuss' examples into a single 'related' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 2. Experiment with more statistics.\n",
    "In today's classifier, we compared the proportion of in-article headline words to the mean of each stance category. Experment with some different statistics. Some ideas:\n",
    "- Use the median instaed of the mean for each stance.\n",
    "- The 'Jaccard index' between two bodies of text, A and B, is (number of words shared by A and B)/(number of words in either A or B). Try using the Jaccard index instead of the proportion of in-article headline words. (hint: look up the functions set(), intersection(), and union())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
