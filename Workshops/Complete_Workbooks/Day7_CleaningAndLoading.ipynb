{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "Today, we're going to work on loading and cleaning the dataset. We'll write a few different functions first, and then combine them together at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning strings\n",
    "\n",
    "First, let's write a function to clean a string. This means taking in a string (word or sentence) and making sure that it is:\n",
    "- All lowercase\n",
    "- All letters and numbers (no symbols!)\n",
    "- Every space is only one space long\n",
    "\n",
    "You will probably find string methods helpful for this task. Take a look at the documentation for Python strings to find some useful methods to accomplish these three tasks:\n",
    "https://docs.python.org/3/library/stdtypes.html#string-methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence should be all lowercase\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out!\n",
    "upper = \"ThIs SeNtenCE sHouLD bE AlL loWErCaSE\"\n",
    "clean_upper = clean(upper)\n",
    "print(clean_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence should have numbers 123 but no symbols\n"
     ]
    }
   ],
   "source": [
    "symbols = \"@this$sentence&should*have?numbers-123-}but|no+symbols#\"\n",
    "clean_symbols = clean(symbols)\n",
    "print(clean_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence should have only one space between words\n"
     ]
    }
   ],
   "source": [
    "spaces = \"this      sentence should    have only  one  space between words\"\n",
    "clean_spaces = clean(spaces)\n",
    "print(clean_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "How do we tokenize a sentence, or break it down into its component words? We can do it ourselves, but there are libraries that do a more advanced job. Let's try making our own function first using string operations. Take a peek at the documentation first:\n",
    "\n",
    "https://docs.python.org/2/library/stdtypes.html#string-methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def our_w_tokenize(s):\n",
    "    # TODO: write a function to split a sentence into its component words\n",
    "    # HINT: look at str.split() in the documentation. Can you get fancier?\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence with words that're regular and that aren't.\n"
     ]
    }
   ],
   "source": [
    "ex_str = \"This is a sentence with words that're regular and that aren't.\"\n",
    "print(our_w_tokenize(ex_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a function using nltk to tokenize a list of words. \n",
    "\n",
    "Compare its output with the output of nltk's word tokenizer. What difference do you see? Why might we want to use a more complex tokenizer? What other kinds of words might be tricky? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A method to tokenize a sentence into words\n",
    "def w_tokenize(s):\n",
    "    return nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sentence', 'with', 'words', 'that', \"'re\", 'regular', 'and', 'that', 'are', \"n't\", '.']\n"
     ]
    }
   ],
   "source": [
    "print(w_tokenize(ex_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tokenize sentences, dividing up a paragraph into sentences. Again, we can write a bunch of rules to do this ourselves, or we can let nltk handle it. Let's try it ourselves, using the same string methods as before. Can you think of examples that might confound your function? How might you approach that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def our_s_tokenize(p):\n",
    "    # TODO: come up with your own simple way of splitting a paragraph into sentences.\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a multi-sentence string (with some unusual parts). Does it handle question marks correctly? What about names like Mr. Rogers? nltk might use different rules than you do!\n"
     ]
    }
   ],
   "source": [
    "ex_paragraph = \"Here is a multi-sentence string (with some unusual parts). Does it handle question marks correctly? What about names like Mr. Rogers? nltk might use different rules than you do!\"\n",
    "print(our_s_tokenize(ex_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that uses the nltk method to tokenize a sentence and compare. What's different? What rules might you have forgotten? Tokenizing is a good example of where rule-based approaches are helpful, but also challenging! It's hard to anticipate every case, but sometimes it's necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to tokenize a paragraph into sentences\n",
    "def s_tokenize(p):\n",
    "    return nltk.sent_tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here is a multi-sentence string (with some unusual parts).', 'Does it handle question marks correctly?', 'What about names like Mr. Rogers?', 'nltk might use different rules than you do!']\n"
     ]
    }
   ],
   "source": [
    "print(s_tokenize(ex_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "Next, we're going to write a function to lemmatize our words. Lemmatizing words means converting them to their most basic form: singular (for nouns), present tense (for verbs), etc.Lemmatizing words makes it easier to compare for content, even if the words don't appear in exactly the same form. We're going to use nltk to lemmatize our words. Take a look at how it works below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "ran\n",
      "run\n",
      "n't\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "## Can you predict what each line will print? Can you predict what each line will print?\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"ran\"))\n",
    "print(lemmatizer.lemmatize(\"ran\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering what the second argument (with pos= and without) is. It's an optional argument that specifies the part of speech --- without it, the lemmatizer assumes everything is a noun. You can try a few examples of your own below if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try whatever words you want here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing this ourselves would be pretty tricky, so we're going to use the nltk lemmatizer. Let's write a function that does the lemmatization on a set of word tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Several', 'people', 'running', 'the', 'marathon', 'were', 'injured', '.']\n"
     ]
    }
   ],
   "source": [
    "# A function to take a list of word tokens and lemmatize each\n",
    "def lemmatize(word_tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokens]\n",
    "    \n",
    "print(lemmatize(w_tokenize(\"Several people running the marathon were injured.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "Can you think about what kinds of words we might not care about when processing natural language for similarity?\n",
    "\n",
    "Words that occur very frequently and don't convey very much information in searches and NLP are called \"stopwords\". You can imagine some examples: the, and, a. We frequently remove these words from search queries and text comparisons to reduce some unecessary noise. Luckily, nltk has our back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'then', 'should', 'doesn', 'as', \"didn't\", 'this', 'herself', 're', \"weren't\", 'its', 'd', 'were', 'under', 'will', 'his', 'your', \"doesn't\", 'which', 'both', 'from', 'yourself', 'itself', 'shouldn', \"that'll\", 'because', 'what', 'a', \"shan't\", \"hadn't\", \"mustn't\", 'are', 's', 'y', 'shan', 'the', 'same', 'aren', 'mightn', 'have', 'mustn', 'until', 'an', 'down', 'before', 'so', \"hasn't\", 'ours', \"you'd\", 'it', 'few', 'himself', 'each', 'them', 't', 'has', 'can', 'than', \"you've\", 'or', 'nor', 'didn', \"you're\", 'yourselves', 'did', \"should've\", 'yours', 'how', 'll', 'haven', 'does', 'any', 'our', 'myself', 'was', 'don', 'wasn', \"mightn't\", 'very', 'once', 'if', 'for', 'you', 'after', 'who', 'above', \"she's\", 'and', 'hasn', \"won't\", 'he', 'now', 'couldn', 'i', 'they', 'of', 'against', 'again', 'theirs', 'why', 'during', 'having', 'here', 'over', 'to', 'ourselves', 'up', 'me', 'had', 'into', 'where', 'is', 'all', \"shouldn't\", 'we', 'my', 'off', \"don't\", 'some', \"haven't\", 'further', 'won', 'these', \"needn't\", \"aren't\", 'that', 'below', 'too', 'needn', 'such', 'be', 'do', 'more', 'their', 'only', 'wouldn', 'not', 'those', 'but', 'between', 'out', 'isn', 'themselves', 'her', 'own', 'she', 'm', 'in', \"it's\", 'with', 'no', 'ma', 'through', 'while', 'on', 'am', 'about', 'o', 'hadn', \"isn't\", 'there', 'just', 'most', 'weren', \"wouldn't\", 'at', 've', 'by', \"wasn't\", \"you'll\", 'ain', 'when', \"couldn't\", 'being', 'other', 'him', 'hers', 'whom', 'been', 'doing'}\n"
     ]
    }
   ],
   "source": [
    "# Here is a list of english stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A method to remove stopwords from sentences.\n",
    "def remove_stopwords(word_tokens):\n",
    "    # TODO: return ONLY the words in word_tokens that DO NOT appear in stop_words\n",
    "    return [w for w in word_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sentence', 'meaningful', 'words', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "stop_ex = remove_stopwords(nltk.word_tokenize(\"This sentence has meaningful words and stopwords\"))\n",
    "print(stop_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together!\n",
    "Let's put together the cleaning methods that we have to clean, tokenize, lemmatize, and remove stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def w_super_clean(s):\n",
    "    return remove_stopwords(lemmatize(w_tokenize(clean(s))))\n",
    "\n",
    "def s_super_clean(p):\n",
    "    sentences = s_tokenize(p)\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "        clean_sentences.append(\" \".join(remove_stopwords(lemmatize(w_tokenize(clean(s))))))\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crazy', 'sentence', 'got', 'lot', 'error']\n"
     ]
    }
   ],
   "source": [
    "# You can create your own \"dirty sentence\" to put your cleaning function to the test\n",
    "dirty_s = \"HeRE's a CRAzy$    sentence that's GOt lots%&*of ERrors\"\n",
    "clean_w = w_super_clean(dirty_s)\n",
    "print(clean_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crazy sentence got lot error', 'one sentence']\n"
     ]
    }
   ],
   "source": [
    "dirty_p = \"HeRE's a CRAzy$    sentence that's GOt lots%&*of ERrors. There iS more*** than ONe SenTence.\"\n",
    "clean_s = s_super_clean(dirty_p)\n",
    "print(clean_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Now, let's try to load in our data so that we can start working with headlines and articles. \n",
    "We're going to load the article bodies into a dictionary, and the headlines and stances into lists of tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function loads a body file and breaks it into words and sentences\n",
    "def load_body(filename):\n",
    "    id2body = {} # TODO: make a dict\n",
    "    id2body_sentences = {} # TODO: make a dict\n",
    "    \n",
    "    # These lines open the file and read in each row\n",
    "    with open(filename, encoding='utf-8', errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        data = list(reader)\n",
    "        for row in data:\n",
    "            \n",
    "            # This line gets the Body ID for this row\n",
    "            id = row['Body ID']\n",
    "            # This line gets the article body\n",
    "            body = str(row['articleBody'])\n",
    "            # This line strips leading and trailing spaces from the body\n",
    "            body = body.strip()\n",
    "            \n",
    "            body_words = w_super_clean(body) # TODO: clean the body words\n",
    "            \n",
    "            body_sentences = s_super_clean(body) # TODO: clean the body sentences\n",
    "            \n",
    "            # TODO: Add this article body to the id2body dict using its id as a key\n",
    "            id2body[id] = body_words\n",
    "            # TODO: Add the list of lists clean_body_sentences to the id2body_sentences dict using its id as a key\n",
    "            id2body_sentences[id] = body_sentences\n",
    "    \n",
    "    return id2body, id2body_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../train_body.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-6209dc3393f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid2body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2body_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../train_body.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2body\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2body_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-3ab4ae667136>\u001b[0m in \u001b[0;36mload_body\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# These lines open the file and read in each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../train_body.csv'"
     ]
    }
   ],
   "source": [
    "id2body, id2body_sentences = load_body(\"../train_body.csv\")\n",
    "print(id2body[0])\n",
    "print(id2body_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_title(filename):\n",
    "    data = # TODO: make a list\n",
    "    \n",
    "    # Open csv and read in rows\n",
    "    with open(filename, errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        raw_data = list(reader)\n",
    "        for row in raw_data:\n",
    "            \n",
    "            body_id = row['Body ID'] #TODO: get the body id cell\n",
    "            title = row['Headline'] #TODO: get the headline cell \n",
    "            title = str(title).strip()\n",
    "            \n",
    "            clean_title = #TODO: clean title\n",
    "        \n",
    "            title_id_tuple = (clean_title, body_id)\n",
    "            # TODO: append a tuple of the title and its id to the data list\n",
    "            \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = load_title(TEST_HEADLINE_CSV)\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_stance(filename):\n",
    "    reader = csv.reader(open(filename, encoding='utf-8', errors='ignore'))\n",
    "    data = []\n",
    "    for title, id, stance in reader:\n",
    "        clean_title = clean(title)\n",
    "        clean_title = get_tokenized_lemmas(clean_title)\n",
    "        data.append((clean_title, id, stance.strip()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2body, id2body_sentences = load_body(TRAIN_BODY_CSV)\n",
    "test_id2body, test_id2body_sentences = load_body(TEST_BODY_CSV)\n",
    "\n",
    "id2body.update(test_id2body)\n",
    "id2body_sentences.update(test_id2body_sentences)\n",
    "\n",
    "train_data = load_stance(TRAIN_STANCE_CSV)[1:]\n",
    "\n",
    "seen_head = set()\n",
    "seen_body_id = set()\n",
    "for (head, body_id, stance) in train_data:\n",
    "    seen_head.add(' '.join(head))\n",
    "    seen_body_id.add(body_id)\n",
    "\n",
    "test_data = load_title(TEST_HEADLINE_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "### Challenge 1: \n",
    "Before, you used string methods to clean strings. The fastest way to do this cleaning is by using regular expressions, which do pattern matching and replacement using a faster algorithm. Using the documentation on the Python package re, can you use a regular expression to do cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(s):\n",
    "    return s #\" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence should be all lowercase\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out!\n",
    "upper = \"ThIs SeNtenCE sHouLD bE AlL loWErCaSE\"\n",
    "cleaner_upper = cleaner(upper)\n",
    "print(cleaner_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence should have numbers 123 but no symbols\n"
     ]
    }
   ],
   "source": [
    "symbols = \"@this$sentence&should*have?numbers-123-}but|no+symbols#\"\n",
    "cleaner_symbols = cleaner(symbols)\n",
    "print(cleaner_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence should have only one space between words\n"
     ]
    }
   ],
   "source": [
    "spaces = \"this      sentence should    have only  one  space between words\"\n",
    "cleaner_spaces = cleaner(spaces)\n",
    "print(cleaner_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2:\n",
    "One of the flaws of our cleaning function is that it doesn't lemmatize non-nouns correctly (because it asks for a part of speech argument). Fortunately, nltk provides a method for part-of-speech tagging. Can you write a new lemmatizing function that tags parts of speech first and uses those tags to do a better job lemmatizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-100-d040263d8606>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-100-d040263d8606>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    word_tags = # TODO: use nltk's pos_tag method to do part-of-speech tagging for the sentence\u001b[0m\n\u001b[0m                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def better_lem(word_tokens):\n",
    "    lemmas = []\n",
    "    word_tags = # TODO: use nltk's pos_tag method to do part-of-speech tagging for the sentence\n",
    "    \n",
    "    for word, tag in word_tags:\n",
    "        if tag.startswith('J'):\n",
    "            pos = wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            pos = wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            pos = wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            pos = wordnet.ADV\n",
    "        else:\n",
    "            pos = ''\n",
    "        # TODO: if pos is not '', add the correct part-of-speech lemma to the lemmas list\n",
    "        # TODO: otherwise, add the noun version (no second argument)\n",
    "    return lemmas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'better_lem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-1ae810ee6642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Verbs like ran run running or be are am is should come out about the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetter_lem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'better_lem' is not defined"
     ]
    }
   ],
   "source": [
    "test_str = \"Verbs like ran run running or be are am is should come out about the same\"\n",
    "print(better_lem(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
