{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors and the FNC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit and download GoogleNews-vectors-negative300.bin.gz. When it finishes downloadin, extract the file into this folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take awhile. While it's downloading, read this page for more information about Word2Vec: https://code.google.com/archive/p/word2vec/. Then, read this more in-depth description: https://www.tensorflow.org/tutorials/representation/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "def initialize():\n",
    "    global word2vec\n",
    "    if len(word2vec) == 0:\n",
    "        print('loading word2vec...')\n",
    "        word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        for word in word_vectors.vocab:\n",
    "            word2vec[normalize_word(word)] = word_vectors[word]\n",
    "        print('word2vec loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec...\n",
      "word2vec loaded\n"
     ]
    }
   ],
   "source": [
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, word2vec is a KeyedVector, and you can view the vectors corresponding to various words. If you're curious, try running the following cell with different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.20703125e-01,   3.27148438e-02,   1.29882812e-01,\n",
       "         2.31933594e-02,  -4.53125000e-01,  -7.51953125e-02,\n",
       "        -9.37500000e-02,   3.90625000e-02,  -1.54296875e-01,\n",
       "        -1.39160156e-02,   2.22167969e-02,  -1.19628906e-01,\n",
       "        -1.68945312e-01,  -2.13623047e-02,  -1.87500000e-01,\n",
       "         1.97753906e-02,   2.22656250e-01,   5.27343750e-01,\n",
       "        -1.21582031e-01,   4.37011719e-02,  -3.71093750e-01,\n",
       "         4.34570312e-02,   7.86132812e-02,  -3.00781250e-01,\n",
       "        -1.56250000e-01,  -2.27539062e-01,  -2.03857422e-02,\n",
       "         1.84326172e-02,   4.37011719e-02,   3.20434570e-03,\n",
       "         2.29492188e-01,   8.00781250e-02,  -1.10839844e-01,\n",
       "        -2.51953125e-01,  -6.05468750e-02,  -4.56542969e-02,\n",
       "        -2.06054688e-01,  -7.71484375e-02,  -5.83496094e-02,\n",
       "        -1.31835938e-01,  -8.39843750e-02,   1.87500000e-01,\n",
       "        -1.34887695e-02,   1.39648438e-01,  -1.77001953e-02,\n",
       "        -1.73828125e-01,  -2.51464844e-02,  -3.16406250e-01,\n",
       "        -1.12792969e-01,   1.20605469e-01,   7.86132812e-02,\n",
       "        -1.43554688e-01,  -5.02929688e-02,   1.30859375e-01,\n",
       "        -3.26538086e-03,   9.47265625e-02,  -6.22558594e-02,\n",
       "        -1.75781250e-02,  -5.20019531e-02,  -2.29492188e-01,\n",
       "         2.70996094e-02,  -2.34375000e-01,  -2.36328125e-01,\n",
       "         1.86523438e-01,   1.00097656e-01,  -1.79687500e-01,\n",
       "        -8.69140625e-02,   9.39941406e-03,   2.26562500e-01,\n",
       "         1.09863281e-02,   1.92382812e-01,   1.26953125e-01,\n",
       "         1.27929688e-01,  -4.00390625e-02,  -2.92968750e-01,\n",
       "         1.33056641e-02,  -4.34875488e-04,   4.73022461e-03,\n",
       "         4.12597656e-02,   3.45703125e-01,   3.22265625e-01,\n",
       "        -1.68945312e-01,  -1.05468750e-01,  -7.37304688e-02,\n",
       "         2.34375000e-01,   3.80859375e-01,  -2.35351562e-01,\n",
       "         4.60815430e-03,  -1.03149414e-02,   9.42382812e-02,\n",
       "        -1.49414062e-01,   1.25976562e-01,   1.54296875e-01,\n",
       "        -4.41894531e-02,  -1.80664062e-01,   2.01171875e-01,\n",
       "         2.59765625e-01,   3.24707031e-02,   5.11718750e-01,\n",
       "         1.17187500e-01,   8.69140625e-02,   2.51953125e-01,\n",
       "         3.30078125e-01,   5.83496094e-02,   8.83789062e-02,\n",
       "         2.21679688e-01,  -8.34960938e-02,  -1.05957031e-01,\n",
       "        -6.88476562e-02,   1.29882812e-01,   3.88183594e-02,\n",
       "        -7.95898438e-02,  -2.92968750e-01,  -5.00488281e-02,\n",
       "         1.51367188e-01,   1.65039062e-01,  -1.92871094e-02,\n",
       "        -2.79296875e-01,   2.23632812e-01,  -2.75390625e-01,\n",
       "        -6.54296875e-02,   7.91015625e-02,  -1.35742188e-01,\n",
       "         6.73828125e-02,   1.04980469e-01,   2.42187500e-01,\n",
       "         8.44726562e-02,   1.79687500e-01,   1.42578125e-01,\n",
       "         1.68945312e-01,   9.27734375e-02,   6.93359375e-02,\n",
       "        -2.10937500e-01,  -2.13867188e-01,  -1.11328125e-01,\n",
       "         3.08837891e-02,  -1.94335938e-01,   1.30859375e-01,\n",
       "        -8.83789062e-02,   2.53906250e-01,  -1.19628906e-01,\n",
       "        -1.15234375e-01,   3.18359375e-01,   4.71191406e-02,\n",
       "         2.83203125e-02,  -1.23535156e-01,  -4.44335938e-02,\n",
       "        -5.40161133e-03,   2.17285156e-02,   1.55273438e-01,\n",
       "         4.22363281e-02,  -2.19726562e-02,   4.54101562e-02,\n",
       "        -3.59375000e-01,  -2.37304688e-01,  -7.66601562e-02,\n",
       "         1.71875000e-01,  -1.53320312e-01,   1.49414062e-01,\n",
       "         2.71484375e-01,   1.15722656e-01,  -2.27050781e-02,\n",
       "         1.54296875e-01,   2.90527344e-02,  -7.42187500e-02,\n",
       "        -4.86328125e-01,   3.90625000e-02,   3.45703125e-01,\n",
       "        -1.85546875e-01,   3.73046875e-01,   3.92578125e-01,\n",
       "         6.98852539e-03,  -5.56640625e-02,  -1.02539062e-01,\n",
       "        -5.66406250e-02,   7.03125000e-02,   1.03027344e-01,\n",
       "        -1.82617188e-01,  -1.40625000e-01,   5.85937500e-02,\n",
       "        -3.88183594e-02,   9.27734375e-02,   3.37890625e-01,\n",
       "         2.26562500e-01,   2.57568359e-02,   3.20312500e-01,\n",
       "         1.02539062e-01,  -9.81445312e-02,   4.95605469e-02,\n",
       "         1.02539062e-01,  -9.08203125e-02,   4.27246094e-02,\n",
       "         2.99072266e-02,   9.27734375e-02,  -1.01562500e-01,\n",
       "         3.19824219e-02,   1.34765625e-01,  -2.28515625e-01,\n",
       "         1.61132812e-01,  -1.27929688e-01,  -1.53320312e-01,\n",
       "         6.00585938e-02,  -2.06298828e-02,  -2.07031250e-01,\n",
       "        -2.75390625e-01,  -2.83203125e-01,   2.09960938e-02,\n",
       "        -1.04980469e-01,  -7.47680664e-03,  -1.46484375e-01,\n",
       "         2.98828125e-01,   5.78613281e-02,  -2.45117188e-01,\n",
       "         2.53906250e-01,  -9.81445312e-02,  -1.56250000e-01,\n",
       "         3.59375000e-01,   3.02734375e-01,  -1.62109375e-01,\n",
       "        -3.82812500e-01,  -1.86767578e-02,   1.70898438e-01,\n",
       "         2.17773438e-01,   1.83593750e-01,  -2.01171875e-01,\n",
       "         3.84765625e-01,  -4.32128906e-02,   3.29589844e-02,\n",
       "        -3.12500000e-02,   1.17675781e-01,  -4.19921875e-01,\n",
       "         7.86132812e-02,  -1.99317932e-04,  -6.59179688e-02,\n",
       "         8.25195312e-02,  -5.88989258e-03,  -4.34875488e-04,\n",
       "         1.77734375e-01,   1.32446289e-02,   6.78710938e-02,\n",
       "         2.61230469e-02,  -1.94335938e-01,   1.81640625e-01,\n",
       "         3.88183594e-02,  -2.67578125e-01,  -7.03125000e-02,\n",
       "        -1.00585938e-01,   6.93359375e-02,   3.61328125e-02,\n",
       "        -1.17675781e-01,   2.32421875e-01,   3.12500000e-02,\n",
       "         2.50000000e-01,  -2.32421875e-01,   2.01171875e-01,\n",
       "         9.76562500e-03,   1.79687500e-01,   1.60156250e-01,\n",
       "         3.39843750e-01,   5.54199219e-02,   4.98046875e-02,\n",
       "        -1.74804688e-01,   6.44531250e-02,  -2.77343750e-01,\n",
       "        -5.63964844e-02,  -1.20117188e-01,  -1.66015625e-01,\n",
       "         3.93066406e-02,   4.92187500e-01,   1.60156250e-01,\n",
       "        -1.25000000e-01,   1.25000000e-01,   6.64062500e-02,\n",
       "        -2.36328125e-01,   2.23388672e-02,   1.32812500e-01,\n",
       "        -2.67578125e-01,  -1.47460938e-01,   2.81250000e-01,\n",
       "         4.57031250e-01,  -1.50390625e-01,  -1.04492188e-01,\n",
       "        -1.69921875e-01,  -6.25000000e-02,   1.34765625e-01,\n",
       "         2.06298828e-02,   8.59375000e-02,  -1.65039062e-01,\n",
       "         1.77001953e-02,   1.47460938e-01,   1.00097656e-01,\n",
       "         6.64062500e-02,   4.95605469e-02,  -1.04980469e-01,\n",
       "        -1.98242188e-01,   2.08740234e-02,  -4.96093750e-01,\n",
       "         1.94549561e-03,  -3.33984375e-01,   2.08984375e-01], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec['hi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a function to convert a sentence into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors we downloaded give us a vector for each word, but we want a vector to represent each sentence. This function does just that. Read through the code, and try to answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Line $2$, why do we write `[0.0] * 300`?** (Hint: try running the following cell and see what happens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.0] * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we need the `if` statement on line $6$? What do you think would happen if we removed it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**What's the point of line $9$ and line $10$?** (Hint: Think about why we divided by the norms of the vectors when computing cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2vector(sentence, word2vec):\n",
    "    vector = np.array([0.0] * 300)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            vector += word2vec[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector /= count\n",
    "        vector /= np.linalg.norm(vector)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try creating vectors from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title1 = \"sam I am\"\n",
    "title2 = \"I am sam\"\n",
    "title3 = \"green eggs and ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titlevector1 = sentence2vector(title1, word2vec)\n",
    "titlevector2 = sentence2vector(title2, word2vec)\n",
    "titlevector3 = sentence2vector(title3, word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cosine similarities between these titles. Do the results surprise you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.667999486431\n"
     ]
    }
   ],
   "source": [
    "print(compute_cosine_similarity(titlevector1, titlevector2))\n",
    "print(compute_cosine_similarity(titlevector1, titlevector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating more sentence vectors of your own choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a function to compute similarity using sentence vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly computing the cosine similarity between pairs of sentences, the FNC solution uses a slightly different similarity metric. This is one part of their semantic_similarities feature (we'll talk a bit more about the other part tomorrow, when we put everything together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the code, and try to answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lines $7$ through $16$ loop through each sentence in `body_sentences`. What does this loop do with each body sentence?** (The `body_sentences` argument is a list of all the sentences in an article body.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the returned feature include?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to re-write this function in pseudo-code, writing a one-line description of what each line of code does.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semantic_similarities(title, body_sentences, word2vec):\n",
    "    title_vector = sentence2vector(title, word2vec)\n",
    "    max_sim = -1\n",
    "    best_vector = np.array([0.0] * 300)\n",
    "\n",
    "    supports = []\n",
    "    for sub_body in body_sentences:\n",
    "        sub_body_vector = sentence2vector(sub_body, word2vec)\n",
    "        similarity = 0\n",
    "        for i in range(300):\n",
    "            similarity += title_vector[i] * sub_body_vector[i]\n",
    "        if similarity > max_sim:\n",
    "            max_sim = similarity\n",
    "            best_vector = sub_body_vector\n",
    "\n",
    "        supports.append(similarity)\n",
    "\n",
    "    features = [max(supports), min(supports)]\n",
    "\n",
    "    for v in best_vector:\n",
    "        features.append(v)\n",
    "    for v in title_vector:\n",
    "        features.append(v)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build upon this code tomorrow, when we start looking at the full FNC solution!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
