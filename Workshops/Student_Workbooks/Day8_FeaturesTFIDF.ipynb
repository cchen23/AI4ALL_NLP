{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction & TF-IDF\n",
    "\n",
    "Today, we're going to implement our tf-idf counter and sketch out the broad outlines of our feature extraction code. Keep in mind, we want everything we write to be compatible with the cleaning and loading code we wrote yesterday, since that's the data that we'll be extracting features from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from math import log\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_body_path = \"train_bodies.csv\"\n",
    "if not os.path.exists(train_body_path):\n",
    "    print(\"Check location for train_bodies\")\n",
    "test_body_path = \"test_bodies.csv\"\n",
    "if not os.path.exists(test_body_path):\n",
    "    print(\"Check location for test_bodies\")\n",
    "train_stance_path = \"train_stances.csv\"\n",
    "if not os.path.exists(train_stance_path):\n",
    "    print(\"Check location for train_stances\")\n",
    "test_headline_path = \"test_stances_unlabeled.csv\"\n",
    "if not os.path.exists(test_headline_path):\n",
    "    print(\"Check location for test_stances_unlabeled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary counting\n",
    "\n",
    "For our idf function, we're going to want to count the number of documents where a word occurs. The best way to do counting of multiple items in Python is using a dictionary where the keys are the items to count and the values are the counts for each item. We're going to practice writing that function first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function will count the items into the dictionary. Count items will be a list (in this case, of words)\n",
    "# and count_dictionary will be a dictionary of counts. It's important to not assume anything about count_dictionary\n",
    "# it could have all keys already in it, or it could be totally empty. \n",
    "def dictionary_count(count_items, count_dictionary):\n",
    "    # TODO: loop through all the items in count_items\n",
    "    for ___ in _______:        \n",
    "        # TODO: if the item is in the dictionary, add one to its current value, the count\n",
    "        \n",
    "        # TODO: if the item isn't in the dictionary, assign it as a key with the value one\n",
    "        \n",
    "    # TODO: return the count dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's test it out\n",
    "\n",
    "fruit_counts = {}\n",
    "my_fruit = [\"apple\", \"blueberry\", \"banana\", \"orange\", \"apple\", \"kiwi\", \"kiwi\", \"strawberry\", \"blueberry\", \"blueberry\"]\n",
    "fruit_counts = dictionary_count(my_fruit, fruit_counts)\n",
    "print(fruit_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating duplicates\n",
    "\n",
    "One more thing we have to do for our idf counting. We want a factor that calculates the number of documents in which a word appears. So, we want to count at most one occurrence of a word per document. What will happen if we just count all occurrences of a word that we see?\n",
    "\n",
    "You can imagine that we'll get a much higher number than we want, since most documents have many repeated words. So, we need to write a function to eliminate duplicate words within a single document (at least, temporarily! We want them in there for frequency counting later). \n",
    "\n",
    "The outline of the function below is doing this from scratch. There are a few ways to accomplish this in Python --- feel free to diverge from the structure and use another strategy if you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes in a list of items and eliminates the duplicates\n",
    "def elim_dupes(items):\n",
    "    # TODO: make a new list\n",
    "\n",
    "    # TODO: loop through all list items\n",
    "\n",
    "        # TODO: if this list item isn't in the new list, add it\n",
    "\n",
    "    # TODO: return the new list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's test it out\n",
    "\n",
    "my_fruit_types = elim_dupes(my_fruit)\n",
    "print(my_fruit_types)\n",
    "\n",
    "single_count_d = {}\n",
    "single_count_d = dictionary_count(my_fruit_types, single_count_d)\n",
    "print(single_count_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF scaling\n",
    "\n",
    "What we're going to do now is write a function that finds the relative frequency of any word token across all documents. We will later use this term to scale individual term counts for each text document.\n",
    "\n",
    "We're going to structure this function to read from a dictionary of text bodies, since that's the format that our id2body data is in. \n",
    "\n",
    "Here is the documentation for the dictionary type. We're going to want a function that lets us loop through the keys and items in a dictionary --- can you find it? \n",
    "\n",
    "https://docs.python.org/3/tutorial/datastructures.html#dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare the idf for a corpus of documents\n",
    "def prepare_idf(corpus):\n",
    "    docs_containing = {}\n",
    "    idf = {}\n",
    "    \n",
    "    # TODO: loop through the items in id2body using a dictionary method\n",
    "    for (body_id, body) in id2body.______:\n",
    "        \n",
    "        # TODO: use your function to remove duplicates from body\n",
    "        \n",
    "        docs_containing = # TODO: use your function to update docs_containing with counts\n",
    "    \n",
    "    for word in docs_containing:\n",
    "        # TODO: set the value in the idf dict for this word to be:\n",
    "        # log (number of total documents / the number of docs that contain the word)\n",
    "        \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's our cleaning code from yesterday! \n",
    "# You don't have to do anything, but read it over and make sure you remember what each function is doing\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "def w_tokenize(s):\n",
    "    return nltk.word_tokenize(s)\n",
    "\n",
    "def s_tokenize(p):\n",
    "    return nltk.sent_tokenize(p)\n",
    "\n",
    "def lemmatize(word_tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokens]\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    # TODO: return ONLY the words in word_tokens that DO NOT appear in stop_words\n",
    "    return [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "def w_super_clean(s):\n",
    "    return remove_stopwords(lemmatize(w_tokenize(clean(s))))\n",
    "\n",
    "def s_super_clean(p):\n",
    "    sentences = s_tokenize(p)\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "        clean_sentences.append(\" \".join(remove_stopwords(lemmatize(w_tokenize(clean(s))))))\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's our load body function from before\n",
    "# Again, you don't need to do anything, but read through and ask if any lines confuse you\n",
    "def load_body(filename):\n",
    "    id2body = {} \n",
    "    id2body_sentences = {} \n",
    "    \n",
    "    # These lines open the file and read in each row\n",
    "    with open(filename, encoding='utf-8', errors='ignore') as fh:\n",
    "        \n",
    "        reader = csv.DictReader(fh)\n",
    "        data = list(reader)\n",
    "        for row in data:\n",
    "            \n",
    "            # This line gets the Body ID for this row\n",
    "            id = row['Body ID']\n",
    "            # This line gets the article body\n",
    "            body = str(row['articleBody'])\n",
    "            # This line strips leading and trailing spaces from the body\n",
    "            body = body.strip()\n",
    "            \n",
    "            # Cleaning words and sentences\n",
    "            body_words = w_super_clean(body) \n",
    "            body_sentences = s_super_clean(body)\n",
    "            \n",
    "            # Adding to the two dictionaries\n",
    "            id2body[id] = body_words\n",
    "            id2body_sentences[id] = body_sentences\n",
    "    \n",
    "    return id2body, id2body_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we're creating the body data that we'll use to train our idf scaler!\n",
    "id2body, id2body_sentences = load_body(train_body_path)\n",
    "test_id2body, test_id2body_sentences = load_body(test_body_path)\n",
    "\n",
    "id2body.update(test_id2body)\n",
    "id2body_sentences.update(test_id2body_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make our idf!\n",
    "idf = prepare_idf(id2body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a peek at some of the entries. Do these look about right to you?\n",
    "print(idf[\"person\"])\n",
    "print(idf[\"dog\"])\n",
    "print(idf[\"goldfish\"])\n",
    "print(idf[\"zebra\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's play with some example sentences\n",
    "ex_s_1 = id2body_sentences['0'][0]\n",
    "print(ex_s_1)\n",
    "\n",
    "ex_s_2 = id2body_sentences['1'][0]\n",
    "print(ex_s_2)\n",
    "\n",
    "ex_s_3 = id2body_sentences['3'][0]\n",
    "print(ex_s_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sentence_idfs(s):\n",
    "    for w in w_tokenize(s):\n",
    "        print(w + \": \" + str(idf[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_sentence_idfs(ex_s_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this what you would have expected? Why or why not? Try running the same function on another sentence and look at those results. Is it what you guessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Forward\n",
    "\n",
    "We're not going to lay out all of our code today, but we're going to look at the broad outlines of our final project code. A code skeleton is a broad outline of your code made out of comments. It's good practice to create a code skeleton before you embark on large projects, so that you can see how everything will fit together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def make_predictions():\n",
    "    \n",
    "    # Load and clean the body (training and test set)\n",
    "    # Load and clean the stances (training set)\n",
    "    # Load and clean the headlines (test set)\n",
    "    \n",
    "    # Prepare the idf\n",
    "    \n",
    "    # Make a predictor to train and predictor\n",
    "    \n",
    "    # For every example in the training set:\n",
    "        # Extract the features\n",
    "        # Do one training step for a predictor using those features and the correct label\n",
    "    \n",
    "    # For every example in the test set:\n",
    "        # Extract the features \n",
    "        # Use the predictor to make a prediction based on those features\n",
    "    \n",
    "    # Check our predicted answers against the real answers\n",
    "    # Output accuracy measures!\n",
    "    \n",
    "# def extract_features():\n",
    "    # Get idf-scaled lexical overlaps \n",
    "    # Get semantic similarity \n",
    "    # Return a vector containing both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we've already done several of the first steps! Ask an instructor if you have any questions at all. Getting features is the next big hurdle, and we'll spend a few days doing that. Great work this week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Max and Min\n",
    "\n",
    "Can you write functions to get the maximum and minimum idf counts for words in a sentence? That will be the rarest and most common word, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_idf(s):\n",
    "    # Hint: split sentence into word tokens\n",
    "    \n",
    "    # Hint: create a variable to hold the maximum idf score and another to hold the word with that score\n",
    "    \n",
    "    # Hint: loop through the word tokens and check each against the maximum score!\n",
    "    return s\n",
    "\n",
    "def get_min_idf(s):\n",
    "    # Hint: split sentence into word tokens\n",
    "    \n",
    "    # Hint: create a variable to hold the minimum idf score and another to hold the word with that score\n",
    "    \n",
    "    # Hint: loop through the word tokens and check each against the minumum score!\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Most common: \" + get_min_idf(ex_s_1) + \"; Least common: \" + get_max_idf(ex_s_1))\n",
    "print(\"Most common: \" + get_min_idf(ex_s_2) + \"; Least common: \" + get_max_idf(ex_s_2))\n",
    "print(\"Most common: \" + get_min_idf(ex_s_3) + \"; Least common: \" + get_max_idf(ex_s_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Synonyms\n",
    "\n",
    "As many of you have mentioned, it would be really cool to be able to check if two words are synonyms when comparing them. NLTK's WordNet allows us to find synsets (synonym sets) which we can use to do just that. Let's try to write a function to check whether two words are synonyms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def synonym_check(word1, word2):\n",
    "    # TODO: get all synsets from word 1\n",
    "\n",
    "        # TODO: get lemmas for this synset\n",
    "\n",
    "            # TODO: compare the name for this lemma to word 2\n",
    "\n",
    "                # TODO: return True if the same \n",
    "\n",
    "    # TODO: otherwise, return false\n",
    "\n",
    "print(synonym_check(\"good\", \"beneficial\"))\n",
    "print(synonym_check(\"bad\", \"negative\"))\n",
    "print(synonym_check(\"many\", \"lots\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get True False False. \n",
    "\n",
    "Hmmm.... we can see that this method isn't as robust as we might like. Another way to check synonyms is to compare similarity indices, and then set a threshold for calling two words synonyms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def synonym_check_2(word1, word2):\n",
    "    # the maximum similarity found so far\n",
    "    max_wup = 0\n",
    "    # gets all possible synsets for word1\n",
    "    w1 = wordnet.synsets(word1)\n",
    "    # gets all possible synsets for word2\n",
    "    w2 = wordnet.synsets(word2) # n denotes noun\n",
    "    # TODO: for synset in w1\n",
    "\n",
    "        # TODO: for synset in w2\n",
    "\n",
    "            # TODO: get wup_similarity between the two\n",
    "\n",
    "            # TODO: if wup_sumilarity is greater than the previous maximum, update it\n",
    "\n",
    "    threshold = # TODO: set threshold\n",
    "    if max_wup > threshold:\n",
    "        return max_wup, True\n",
    "    else:\n",
    "        return max_wup, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(synonym_check_2(\"good\", \"beneficial\"))\n",
    "print(synonym_check_2(\"bad\", \"negative\"))\n",
    "print(synonym_check_2(\"horse\", \"goat\"))\n",
    "print(synonym_check_2(\"terrible\", \"horrible\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method doesn't work very well either! Can you come up with something better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synonym_check_3(word1, word2):\n",
    "    # TODO: your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
