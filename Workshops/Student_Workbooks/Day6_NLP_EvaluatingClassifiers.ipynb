{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to discuss metrics for comparing classifiers. We will bring back the rule-based classifier that was introduced yesterday (the classifier that used the word overlap proportion for headlines and articles in order to make its decision; although we will restrict this classifier so that it can only predict 'unrelated' and 'related'), and we will compare the performance of this classifier against a classifier that classifies every article-headline pair as 'unrelated'. We will see that accuracy is only one way to measure performance, and that it may not be the best metric when the training and test examples are unbalanced across classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anytime you see ``______ # TODO: FILL IN HERE.`` in the code, you should replace the ``______`` with your own code.\n",
    "\n",
    "As always, ask your neighbors or an instructor if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have copied the functions that we created yesterday into a separate file (named 'initial_classifier_utils.py'), and we will now import those functions here, along with the other python packages we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initial_classifier_utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os.path\n",
    "# Adjust settings so that we can fully see the dataset below\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data, if it doesn't already exist:\n",
    "if os.path.isfile('train_data.csv') == False:\n",
    "    utils.merge_data(\"train_stances.csv\", \"train_bodies.csv\", \"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test data, if it doesn't already exist:\n",
    "if os.path.isfile('test_data.csv') == False:\n",
    "    utils.merge_data(\"competition_test_stances.csv\", \"competition_test_bodies.csv\", \"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training data and separate out the training data according to the value of the Stance variable:\n",
    "train_data = pd.read_csv(\"train_data.csv\", encoding = \"utf-8\")\n",
    "unrelated_train = train_data[train_data['Stance'] == 'unrelated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE\n",
    "discuss_train = train_data[train_data['Stance'] == ________]  # TODO: FILL IN HERE\n",
    "agree_train = train_data[train_data[______] == 'agree'] # TODO: FILL IN HERE\n",
    "disagree_train = _______[______['Stance'] == 'disagree'] # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test data:\n",
    "# TODO: FILL IN HERE \n",
    "test_data = pd.______(\"test_data.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the classifier and make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to compute the headline-article word overlap for each of the examples in the unrelated,\n",
    "# discuss, agree and disagree categories in the training set.  \n",
    "# Warning: this may take a minute - it is iterating through almost 50,000 examples!\n",
    "proportions_train = utils.compute_proportions(unrelated_train, discuss_train, agree_train, disagree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to modify the make_prediction function that we created yesterday, so that it only predicts 'unrelated'\n",
    "# and 'related'. It will use the same method as our classifier yesterday used: the proportion overlap for an example\n",
    "# will be compared with the overlap proportions for 'related' and 'unrelated' before a decision is made\n",
    "def make_prediction(example, proportions_train):\n",
    "    # Keep only the 'unrelated' and 'related' proportions:\n",
    "    keys = ['unrelated', 'related']\n",
    "    new_proportions = { key: proportions_train[key] for key in keys }\n",
    "    proportions_stances = list(new_proportions.keys())\n",
    "    proportion = utils.find_headline_in_article_proportion(example)\n",
    "    predicted_stance = proportions_stances[np.argmin(np.abs(np.array(list(new_proportions.values())) - proportion))]\n",
    "    return predicted_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a lot of the functions that were created yesterday, but we are also going to define a new one:\n",
    "# The function below takes the test data set and iterates through each article-headline pair. For each article-headline pair\n",
    "# it makes a prediction by calculating the headline-article word overlap and comparing this to the mean overlap values \n",
    "# for each of the categories in the training set. The function returns a list of predictions for every example in the\n",
    "# test data set\n",
    "def make_predictions(test_data, proportions_train):\n",
    "    predictions_list = []\n",
    "    for i in range(test_data.shape[0]):\n",
    "        example = test_data.iloc[i]\n",
    "        predicted_stance = make_prediction(example, proportions_train)\n",
    "        # Append the predicted stance to the predictions_list - this will allow us to compare predictions and true \n",
    "        # values later on\n",
    "        predictions_list.append(predicted_stance)\n",
    "    return predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the make_predictions function (it may take a minute, as the test dataset contains a lot of examples!) \n",
    "predictions = make_predictions(test_data, proportions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE\n",
    "# Check that the predictions list is the right size (this one way to check that the make_predictions function \n",
    "# is performing as expected): \n",
    "# 1. From test_data, what do you expect the length of the predictions list to be? \n",
    "# (You may want to calculate an attribute of test_data you have seen on a previous day)\n",
    "# 2. What is the length of the predictions list and does it match your expectation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare predictions and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a new column (called 'new_stance') in the test_data dataframe so that any Stance value that is \n",
    "# 'agree', 'discuss', or'disagree' is represented as 'related', while a value of 'unrelated' remains 'unrelated'\n",
    "test_data['new_stance'] = test_data['Stance']\n",
    "test_data.loc[test_data['Stance'].isin(['agree', 'disagree', 'discuss']), 'new_stance'] = 'related'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to append the predictions list we just obtained to the test_data dataframe as an additional column,\n",
    "# along with one more column that predicts 'unrelated' for every example:\n",
    "test_data['prediction_1'] = predictions\n",
    "test_data['prediction_2'] = 'unrelated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE\n",
    "# Let's look at the modified test_data dataframe by viewing the first few examples: (use a function you have learned\n",
    "# about on a previous day)\n",
    "test_data._____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a few more examples (this time at the end of the dataset):\n",
    "test_data[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compare classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to introduce some tools that we can use for evaluating classifier performance. These are:\n",
    "- Accuracy \n",
    "- Confusion Matrices\n",
    "- Precision and Recall\n",
    "- F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is defined as the ratio of the number of examples that were correctly predicted compared to the number of examples in the test dataset:\n",
    "$\\text{accuracy} = \\dfrac{\\text{number correct}}{\\text{number of examples}}$\n",
    "\n",
    "Let's calculate accuracy values for the classifier we saw yesterday, and compare this to the classifier which predicts 'unrelated' for all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(test_data, truth_col_name, prediction_col_name):\n",
    "    number_correct = sum(test_data[truth_col_name] == test_data[prediction_col_name])\n",
    "    number_examples = test_data.shape[0]\n",
    "    accuracy = number_correct/number_examples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for the classifier we created yesterday:\n",
    "calculate_accuracy(test_data, 'new_stance', 'prediction_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE\n",
    "# Calculate accuracy for the classifier that classifies all examples as 'unrelated':\n",
    "calculate_accuracy(test_data, 'new_stance', _______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just seen that the classifier that predicts 'unrelated' for everything manages to obtain a high accuracy value. But there is something unsatisfying about this second classifier, and if we were using it to predict articles that might be Fake News (because the headline and article are unrelated), all articles would be marked as Fake News! This doesn't seem helpful! \n",
    "\n",
    "Perhaps we should be using other tools, in addition to accuracy, to evaluate our classifier..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to calculate something called a 'confusion matrix' for each of our classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(test_data, truth_col_name, prediction_col_name):\n",
    "    cross_tab = pd.crosstab(test_data[truth_col_name], test_data[prediction_col_name])\n",
    "    column_names = cross_tab.columns.values\n",
    "    row_names = cross_tab.index\n",
    "    # Check each row name has an equivalent column; if not, add column and fill with 0s\n",
    "    for row in row_names:\n",
    "        if row not in column_names:\n",
    "            cross_tab[row] = 0\n",
    "    # reorder columns so that order matches that of rows and return this\n",
    "    return cross_tab[row_names] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_mat_1 = calculate_confusion_matrix(test_data, 'new_stance', 'prediction_1')\n",
    "cf_mat_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the confusion matrix tell us? \n",
    "\n",
    "If we look at the first entry of the confusion matrix, with value 4491, this tells us that 4491 examples in the test dataset had label 'related' (row value) and were correctly classified by our classifier as 'related' (column value).\n",
    "\n",
    "If we look at the entry in the second row and first column (value 1819), this tells us that 1819 examples in the test dataset had label 'unrelated', but were classified as 'related' by our classifier. Similarly, 2573 examples had true label 'related' but were classified as 'unrelated' by our classifier; and 16,530 examples had true label 'unrelated' and were correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE\n",
    "# Let's also look at the confusion matrix for the second classifier:\n",
    "cf_mat_2 = calculate_confusion_matrix(test_data, 'new_stance', ___________)\n",
    "cf_mat_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, when trying to interpret the confusion matrix, it helps to divide the entries in the confusion matrix by either the number of test examples altogether, or by the number of examples in each class (i.e. divide each entry in the matrix by the sum of the elements in its row). Let's do the latter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_accuracies(confusion_matrix):\n",
    "    # Calculate row sum for confusion matrix (total number of examples in a particular class):\n",
    "    row_sums = confusion_matrix.sum(1)\n",
    "    # Divide each element by its row sum to calculate class accuracies:\n",
    "    class_acc_mat = confusion_matrix.divide(row_sums, axis = 0)\n",
    "    # Add column which sums row proportions\n",
    "    class_acc_mat['row_sum'] = class_acc_mat.sum(1)\n",
    "    return class_acc_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_acc_mat_1 = calculate_class_accuracies(cf_mat_1)\n",
    "class_acc_mat_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the scaled confusion matrix above tell us? If we look at the first entry (0.635759), this tells us that ~64% of the examples in our test dataset that were marked as 'related' were properly classified by our classifier, while 36% were incorrectly classified (entry in first row, second column). In comparison, 90% of the 'unrelated' examples in the test dataset were correctly classified, while ~10% were incorrectly classified. This means that our classifier is better at identifying 'unrelated' examples compared to 'related' examples, and that we should think about other features we can use to distinguish 'related' examples from 'unrelated' examples.\n",
    "\n",
    "What happens when we calculate the class accuracies for the second classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE\n",
    "# What happens when we calculate the class accuracies for the second classifier?\n",
    "class_acc_mat_2 = calculate_class_accuracies(_______)\n",
    "class_acc_mat_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we calculate the scaled confusion matrix here, we see that, as expected, 100% of 'unrelated' examples are correctly classified, while 100% of 'related' examples are incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, we can calculate some additional quantities. In particular, we can calculate **precision** and **recall** scores for our classifiers. In our case, precision is defined as:\n",
    "\n",
    "$\\text{precision} = \\dfrac{\\text{Number of unrelated examples correctly classified}}{\\text{Number of examples classified as 'unrelated'}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(confusion_matrix):\n",
    "    unrelated_correct = confusion_matrix[confusion_matrix.index == 'unrelated']['unrelated']\n",
    "    classified_as_unrelated = confusion_matrix.sum(0)['unrelated']\n",
    "    precision = (unrelated_correct/classified_as_unrelated)[0]\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now calculate the precision value for the first classifier with its confusion matrix:\n",
    "calculate_precision(cf_mat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE\n",
    "# Let's do the same for our second classifier:\n",
    "calculate__________(cf_mat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is defined as:\n",
    "$\\text{recall} = \\dfrac{\\text{Number of unrelated examples correctly classified}}{\\text{Number of unrelated examples}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(confusion_matrix):\n",
    "    unrelated_correct = confusion_matrix[confusion_matrix.index == 'unrelated']['unrelated']\n",
    "    number_unrelated = confusion_matrix.sum(1)['unrelated']\n",
    "    recall = (unrelated_correct/number_unrelated)[0]\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now calculate the recall value for the first classifier with its confusion matrix:\n",
    "calculate_recall(cf_mat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE\n",
    "# Let's now calculate the recall value for the second classifier with its confusion matrix:\n",
    "__________(cf_mat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have introduced **accuracy**, **confusion matrices**, **precision** and **recall** as tools for evaluating classifier performance. We have compared two possible classifiers for the Fake News Challenge, and have seen that, while one classifier may get a higher score according to one metric, it may get a lower score when using a different metric (our classifier from yesterday returns higher accuracy and precision scores compared to the classifier that labels everything as 'unrelated', but has a lower recall score). The metric that is used to evaluate classifier performance is often specific to the nature of the problem being studied, and often multiple metrics are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extra Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take a look at some of the examples that were misclassified by our proportion overlap classifier. Why do you think our proportion overlap model failed? Can you think of a rule that would properly classify these examples?  Why not test it out?\n",
    "2. Yesterday we created a four-way classifier that was able to predict 'agree', 'disagree', 'discuss' and 'unrelated'. Calculate the confusion matrix for this classifier. In this case, how many rows and columns would the matrix have? Compare the confusion matrix for this classifier to the classifier that predicts 'unrelated' for everything. What does the confusion matrix tell us about each of these models? What are the strengths and weaknesses of both of these models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
