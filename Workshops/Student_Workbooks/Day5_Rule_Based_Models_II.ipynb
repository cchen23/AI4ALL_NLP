{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll create some rule-based classifiers. In this workbook, we'll practice working with Python and the FNC data, and create a simple FNC predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Anytime you see ``______ # TODO: FILL IN HERE.`` in the code, you should replace the ``______`` with your own code. ***\n",
    "\n",
    "As always, ask your neighbors or an instructor if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There are some longer functions here, where many of the lines have already been written. Try to read through the code and understand why we're doing each step!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import the packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have ``train_data.csv`` and ``test_data.csv`` saved, run the following code to construct and save csvs of the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_data(stances_filename, bodies_filename, merged_filename):\n",
    "    stances = pd.read_csv(stances_filename, encoding = \"utf-8\")\n",
    "    bodies = pd.read_csv(bodies_filename, encoding = \"utf-8\")\n",
    "    data = pd.merge(bodies, stances, on='Body ID')\n",
    "    data.to_csv(merged_filename, index=False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where are the train_stances.csv and train_bodies.csv files?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_data(\"../train_stances.csv\", \"../train_bodies.csv\", \"../train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_data(\"../competition_test_stances.csv\", \"../competition_test_bodies.csv\", \"../test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we add a \"../\" in front of the names of our .csv files?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to find all six .csv files in Finder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Separate the dataset based on stance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in and run the following cells to separate the train and test sets based on their stance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../train_data.csv\", encoding = \"utf-8\")\n",
    "unrelated_train = train_data[train_data['Stance'] == ____] # TODO: Fill in here.\n",
    "discuss_train = train_data[train_data['Stance'] == ____] # TODO: Fill in here.\n",
    "agree_train = train_data[train_data['Stance'] == ____] # TODO: Fill in here.\n",
    "disagree_train = train_data[train_data['Stance'] == ____] # TODO: Fill in here.\n",
    "\n",
    "test_data = pd.read_csv(\"../test_data.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a classifer based on shared words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first classifier will be based on the percentage of headline words that appear in the article. We'll find the average percentage for each stance. When our classifier gets a headline-article pair, it'll find the percentage of headline words in the article and then predict the stance with the closest average percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, try to write out the steps of this classifier, and the components we'll need to write to create the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Write a function that finds the proportion of headline words in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_headline_in_article_proportion(example):\n",
    "    headline_words = str.split(example['Headline'])\n",
    "    article_words = str.split(example[____])  # TODO: Fill in here.\n",
    "    counter = 0\n",
    "    for word in headline_words:\n",
    "        if word in article_words:\n",
    "            counter += ____ # TODO: Fill in here.\n",
    "    proportion = ____/len(____)\n",
    "    return proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain in plain English what this function does:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out this function. Before running the following code, try to predict what the output will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline_text = \"this is a test headline\"\n",
    "article_text = \"this is a test article\"\n",
    "test_df = pd.DataFrame({'Headline':[headline_text], 'articleBody':[article_text]})\n",
    "example = test_df.iloc[0]\n",
    "find_headline_in_article_proportion(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test it out on a headline and article pair of your choosing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline_text = ____  # TODO: Fill in here.\n",
    "article_text = ____  # TODO: Fill in here.\n",
    "test_df = pd.DataFrame({'Headline':[headline_text], 'articleBody':[article_text]})\n",
    "example = test_df.iloc[0]\n",
    "find_headline_in_article_proportion(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test out the function on an example from the FNC data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = train_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_headline_in_article_proportion(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we wanted to use something other than the `print` function, how could we have seen what `example` contains?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Find the average proportion for each stance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will find the average proportion overlap between article and body for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_proportions(unrelated, discuss, agree, disagree):\n",
    "    proportions_unrelated = []\n",
    "    for i in range(unrelated.shape[0]):\n",
    "        this_example = unrelated.iloc[i]\n",
    "        proportions_unrelated.append(find_headline_in_article_proportion(this_example))\n",
    "    proportions_related = []\n",
    "    proportions_discuss= []\n",
    "    for i in range(discuss.shape[0]):\n",
    "        this_example = discuss.iloc[i]\n",
    "        proportions_discuss.append(find_headline_in_article_proportion(this_example))\n",
    "        proportions_related.append(find_headline_in_article_proportion(this_example))\n",
    "    proportions_agree= []\n",
    "    for i in range(agree.shape[0]):\n",
    "        this_example = agree.iloc[i]\n",
    "        proportions_agree.append(find_headline_in_article_proportion(this_example))\n",
    "        proportions_related.append(find_headline_in_article_proportion(this_example))\n",
    "    proportions_disagree= []\n",
    "    for i in range(disagree.shape[0]):\n",
    "        this_example = disagree.iloc[i]\n",
    "        proportions_disagree.append(find_headline_in_article_proportion(this_example))\n",
    "        proportions_related.append(find_headline_in_article_proportion(this_example))\n",
    "    return {\"unrelated\":np.mean(proportions_unrelated), \"discuss\":np.mean(proportions_discuss), \"agree\":np.mean(proportions_agree), \"disagree\":np.mean(proportions_disagree), \"related\":np.mean(proportions_related)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proportions = compute_proportions(unrelated_train, discuss_train, agree_train, disagree_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at `proportions`, try to guess what the proportions are for each category.\n",
    "Side note: We saved proportions in a **dictionary**. If you're interested in learning more about it, ask an instructor or see this description: https://www.tutorialspoint.com/python/python_dictionary.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now look at the proportions for each category.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Write a prediction function based on the closest average proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll write a function that predicts an example's class depending on which group's overlap proportion it's closest to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(example, proportions):\n",
    "    proportions_stances = list(proportions.keys())\n",
    "    proportion = find_headline_in_article_proportion(____) # TODO: Fill in here.\n",
    "    predicted_stance = proportions_stances[np.argmin(np.abs(np.array(list(proportions.values())) - proportion))]\n",
    "    return predicted_stance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a.  Write a function that runs and evaluates predictions on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function classifies the percentage of correct predictions for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_predictions(test_data, proportions):\n",
    "    stance_counts = {\"unrelated\":0, \"discuss\":0, \"agree\":0, \"disagree\":0}\n",
    "    stance_correct_counts = {\"unrelated\":0, \"discuss\":0, \"agree\":0, \"disagree\":0}\n",
    "    for i in range(test_data.shape[0]):\n",
    "        example = test_data.iloc[i]\n",
    "        predicted_stance = make_prediction(____, ____)  # TODO: Fill in here.\n",
    "        actual_stance = example[____]  # TODO: Fill in here.\n",
    "        stance_counts[actual_stance] += ____  # TODO: Fill in here.\n",
    "        if predicted_stance == actual_stance:\n",
    "            stance_correct_counts[actual_stance] += ____  # TODO: Fill in here.\n",
    "    return {\"unrelated\":stance_correct_counts[\"unrelated\"]/stance_counts[\"unrelated\"], \"discuss\":stance_correct_counts[\"discuss\"]/stance_counts[\"discuss\"], \"agree\":stance_correct_counts[\"agree\"]/stance_counts[\"agree\"], \"disagree\":stance_correct_counts[\"disagree\"]/stance_counts[\"disagree\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Test the classifier and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_percentages = test_predictions(____, ____)  # TODO: Fill in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What categories does the predictor do well on, and what does the predictor do less well on? Why do you think this is? (hint: take a look at the average proportion of in-article headline words).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does very well on 'unrelated' and worst on 'discuss'. The mean proportion for 'unrelated' is very different from the other categories, while the 'discuss' proportion is sandwiched between the proportions for 'agree' and 'disagree'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the number of examples in each category, and compute the overall classification accuracy. Is this higher or lower than you would have expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrelated_test = test_data[test_data['Stance'] == ____] # TODO: Fill in here.\n",
    "discuss_test = test_data[test_data['Stance'] == ____] # TODO: Fill in here.\n",
    "agree_test = test_data[test_data['Stance'] == ____] # TODO: Fill in here.\n",
    "disagree_test = test_data[test_data['Stance'] == ____] # TODO: Fill in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_accuracy = (correct_percentages[____] * ____.shape[0] \\ # TODO: Fill in here.\n",
    "                    + correct_percentages[____] * ____.shape[0] \\ # TODO: Fill in here.\n",
    "                    + correct_percentages[____] * ____.shape[0] \\ # TODO: Fill in here.\n",
    "                    + correct_percentages[____] * ____.shape[0]) \\ # TODO: Fill in here.\n",
    "                    / ____.shape[0] # TODO: Fill in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you think of this accuracy? How much better do you think we can do?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a simple classifer based on the most common category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5a. Find the most common category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find what the most common category is, using the training data. For something like this, there's often a function someone else used that we can borrow. The easiest way to find this is by googling -- for example, I googled \"pandas series count values for each type\" and this was the second result: https://stackoverflow.com/questions/22391433/count-the-frequency-that-a-value-occurs-in-a-dataframe-column. Check out this link, and try to use one line to find the number of occurrences of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "____  # TODO: Fill in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the most common category?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extra Challenge: Try determining the number of examples for each category without using these functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b. Compute the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find how many examples of each category there are in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "____ # TODO: Fill in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, what would the accuracy be if we always guessed the most common category?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "____/____.shape[0] # TODO: Fill in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Extra challenge\" sections are a more unguided exploration into the concepts we've discussed. You'll notice less scaffolding for the code -- try implementing these concepts from scratch, and feel free to ask your neighbors or an instructor if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1. Two-way classification.\n",
    "\n",
    "Try adapting our code for four-way classification (between 'unrelated', 'agree', 'disagree', and 'discuss') for two-way classification (between 'unrelated' and 'related'). To do this, we'll group 'agree', 'disagree', and 'discuss' examples into a single 'related' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 2. Experiment with more statistics.\n",
    "In today's classifier, we compared the proportion of in-article headline words to the mean of each stance category. Experment with some different statistics. Some ideas:\n",
    "- Use the median instaed of the mean for each stance.\n",
    "- The 'Jaccard index' between two bodies of text, A and B, is (number of words shared by A and B)/(number of words in either A or B). Try using the Jaccard index instead of the proportion of in-article headline words. (hint: look up the functions set(), intersection(), and union())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
