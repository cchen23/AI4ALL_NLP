{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors and the FNC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit and download GoogleNews-vectors-negative300.bin.gz. When it finishes downloadin, extract the file into this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take awhile. While it's downloading, read this page for more information about Word2Vec: https://code.google.com/archive/p/word2vec/. Then, read this more in-depth description: https://www.tensorflow.org/tutorials/representation/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "def initialize():\n",
    "    global word2vec\n",
    "    if len(word2vec) == 0:\n",
    "        print('loading word2vec...')\n",
    "        word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        for word in word_vectors.vocab:\n",
    "            word2vec[normalize_word(word)] = word_vectors[word]\n",
    "        print('word2vec loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, word2vec is a KeyedVector, and you can view the vectors corresponding to various words. If you're curious, try running the following cell with different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec['hi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a function to convert a sentence into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors we downloaded give us a vector for each word, but we want a vector to represent each sentence. This function does just that. Read through the code, and try to answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Line $2$, why do we write `[0.0] * 300`?** (Hint: try running the following cell and see what happens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[0.0] * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we need the `if` statement on line $6$? What do you think would happen if we removed it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**What's the point of line $9$ and line $10$?** (Hint: Think about why we divided by the norms of the vectors when computing cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2vector(sentence, word2vec):\n",
    "    vector = np.array([0.0] * 300)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            vector += word2vec[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector /= count\n",
    "        vector /= np.linalg.norm(vector)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try creating vectors from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title1 = \"sam I am\"\n",
    "title2 = \"I am sam\"\n",
    "title3 = \"green eggs and ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titlevector1 = sentence2vector(____, ____) # TODO: FILL IN HERE.\n",
    "titlevector2 = sentence2vector(____, ____) # TODO: FILL IN HERE.\n",
    "titlevector3 = sentence2vector(____, ____) # TODO: FILL IN HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cosine similarities between these titles. Do the results surprise you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(compute_cosine_similarity(titlevector1, titlevector2))\n",
    "print(compute_cosine_similarity(titlevector1, titlevector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating more sentence vectors of your own choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_a = ____ # TODO: FILL IN HERE\n",
    "title_B = ____ # TODO: FILL IN HERE\n",
    "titlevector_a = sentence2vector(____, ____) # TODO: FILL IN HERE.\n",
    "titlevector_b = sentence2vector(____, ____) # TODO: FILL IN HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(compute_cosine_similarity(titlevector_a, titlevector_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a function to compute similarity using sentence vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly computing the cosine similarity between pairs of sentences, the FNC solution uses a slightly different similarity metric. This is one part of their semantic_similarities feature (we'll talk a bit more about the other part tomorrow, when we put everything together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the code, and try to answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lines $7$ through $16$ loop through each sentence in `body_sentences`. What does this loop do with each body sentence?** (The `body_sentences` argument is a list of all the sentences in an article body.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the returned feature include?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to re-write this function in pseudo-code, writing a one-line description of what each line of code does.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semantic_similarities(title, body_sentences, word2vec):\n",
    "    title_vector = sentence2vector(title, word2vec)\n",
    "    max_sim = -1\n",
    "    best_vector = np.array([0.0] * 300)\n",
    "\n",
    "    supports = []\n",
    "    for sub_body in body_sentences:\n",
    "        sub_body_vector = sentence2vector(sub_body, word2vec)\n",
    "        similarity = 0\n",
    "        for i in range(300):\n",
    "            similarity += title_vector[i] * sub_body_vector[i]\n",
    "        if similarity > max_sim:\n",
    "            max_sim = similarity\n",
    "            best_vector = sub_body_vector\n",
    "\n",
    "        supports.append(similarity)\n",
    "\n",
    "    features = [max(supports), min(supports)]\n",
    "\n",
    "    for v in best_vector:\n",
    "        features.append(v)\n",
    "    for v in title_vector:\n",
    "        features.append(v)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll build upon this code tomorrow, when we start looking at the full FNC solution!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
