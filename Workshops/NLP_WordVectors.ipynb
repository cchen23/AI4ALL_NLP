{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anytime you see ``______ # TODO: FILL IN HERE.`` in the code, you should replace the ``______`` with your own code.\n",
    "\n",
    "As always, ask your neighbors or an instructor if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Word Vectors\n",
    "Let's start by working with pre-trained word vectors. These are word representations that have been created by other people, which we can download and use for our applications.\n",
    "\n",
    "We'll work with vectors trained using the Word2Vec model. Other people have already created these vectors, which are available to download online. Don't worry about the specific methods used to create these vectors for this class, but if you're interested in learning more feel free to ask one of the instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  0. Import packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the packages we need in the rest of this notebook.\n",
    "\n",
    "As a reminder, press ctrl-enter to run a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download pre-trained word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Download the data online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the word vectors, go to this link: https://nlp.stanford.edu/projects/glove/\n",
    "and search for \"glove.6B.zip\".\n",
    "\n",
    "Download this file, and save it in the same folder as this notebook.\n",
    "\n",
    "Then extract the files contained in the .zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Load the word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided the \"read_embeddings\" function, which takes in the name of one of the embedding files and reads it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two cells to create the ``read_embeddings`` function and load a set of embeddings into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embeddings(filename):\n",
    "    word_embeddings = pd.read_table(filename, header=None, sep=\" \", index_col=0, quoting=3)\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_file = 'glove.6B/glove.6B.200d.txt'\n",
    "embeddings_df = read_embeddings(embeddings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Look at some examples of word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually examine the first few rows of a DataFrame, we can use the ``.head()`` function of the DataFrame.\n",
    "\n",
    "In this case, we named the DataFrame ``embeddings_df``, so we run ``embeddings_df.head()`` to view the first few rows.\n",
    "\n",
    "Run the following cell to see the first few embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.071549</td>\n",
       "      <td>0.093459</td>\n",
       "      <td>0.023738</td>\n",
       "      <td>-0.090339</td>\n",
       "      <td>0.056123</td>\n",
       "      <td>0.32547</td>\n",
       "      <td>-0.39796</td>\n",
       "      <td>-0.092139</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>-0.189500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.19957</td>\n",
       "      <td>-0.20303</td>\n",
       "      <td>0.344740</td>\n",
       "      <td>-0.243280</td>\n",
       "      <td>0.131390</td>\n",
       "      <td>-0.008877</td>\n",
       "      <td>0.336170</td>\n",
       "      <td>0.030591</td>\n",
       "      <td>0.255770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.176510</td>\n",
       "      <td>0.292080</td>\n",
       "      <td>-0.002077</td>\n",
       "      <td>-0.375230</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.23979</td>\n",
       "      <td>-0.28893</td>\n",
       "      <td>-0.014643</td>\n",
       "      <td>-0.109930</td>\n",
       "      <td>0.155920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.325820</td>\n",
       "      <td>0.19153</td>\n",
       "      <td>-0.15469</td>\n",
       "      <td>-0.146790</td>\n",
       "      <td>0.046971</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>-0.220060</td>\n",
       "      <td>-0.207740</td>\n",
       "      <td>-0.231890</td>\n",
       "      <td>-0.108140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.122890</td>\n",
       "      <td>0.580370</td>\n",
       "      <td>-0.069635</td>\n",
       "      <td>-0.502880</td>\n",
       "      <td>0.105030</td>\n",
       "      <td>0.39945</td>\n",
       "      <td>-0.38635</td>\n",
       "      <td>-0.084279</td>\n",
       "      <td>0.122190</td>\n",
       "      <td>0.080312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035236</td>\n",
       "      <td>0.17688</td>\n",
       "      <td>-0.05360</td>\n",
       "      <td>0.007003</td>\n",
       "      <td>-0.033006</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.244510</td>\n",
       "      <td>-0.039174</td>\n",
       "      <td>-0.162360</td>\n",
       "      <td>-0.096652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.052924</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.313530</td>\n",
       "      <td>-0.356130</td>\n",
       "      <td>0.029629</td>\n",
       "      <td>0.51034</td>\n",
       "      <td>-0.10716</td>\n",
       "      <td>0.151950</td>\n",
       "      <td>0.057698</td>\n",
       "      <td>0.061490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040886</td>\n",
       "      <td>0.38940</td>\n",
       "      <td>-0.10509</td>\n",
       "      <td>0.233720</td>\n",
       "      <td>0.096027</td>\n",
       "      <td>-0.303240</td>\n",
       "      <td>0.244880</td>\n",
       "      <td>-0.086254</td>\n",
       "      <td>-0.419170</td>\n",
       "      <td>0.464960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.573460</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>-0.234770</td>\n",
       "      <td>-0.362400</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.11386</td>\n",
       "      <td>-0.44933</td>\n",
       "      <td>-0.309910</td>\n",
       "      <td>-0.005341</td>\n",
       "      <td>0.584260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279150</td>\n",
       "      <td>0.43742</td>\n",
       "      <td>-0.31237</td>\n",
       "      <td>0.131940</td>\n",
       "      <td>-0.332780</td>\n",
       "      <td>0.188770</td>\n",
       "      <td>-0.234220</td>\n",
       "      <td>0.544180</td>\n",
       "      <td>-0.230690</td>\n",
       "      <td>0.349470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5        6        7    \\\n",
       "0                                                                         \n",
       "the -0.071549  0.093459  0.023738 -0.090339  0.056123  0.32547 -0.39796   \n",
       ",    0.176510  0.292080 -0.002077 -0.375230  0.004914  0.23979 -0.28893   \n",
       ".    0.122890  0.580370 -0.069635 -0.502880  0.105030  0.39945 -0.38635   \n",
       "of   0.052924  0.254270  0.313530 -0.356130  0.029629  0.51034 -0.10716   \n",
       "to   0.573460  0.541700 -0.234770 -0.362400  0.403700  0.11386 -0.44933   \n",
       "\n",
       "          8         9         10     ...          191      192      193  \\\n",
       "0                                    ...                                  \n",
       "the -0.092139  0.061181 -0.189500    ...     0.121800  0.19957 -0.20303   \n",
       ",   -0.014643 -0.109930  0.155920    ...    -0.325820  0.19153 -0.15469   \n",
       ".   -0.084279  0.122190  0.080312    ...    -0.035236  0.17688 -0.05360   \n",
       "of   0.151950  0.057698  0.061490    ...    -0.040886  0.38940 -0.10509   \n",
       "to  -0.309910 -0.005341  0.584260    ...    -0.279150  0.43742 -0.31237   \n",
       "\n",
       "          194       195       196       197       198       199       200  \n",
       "0                                                                          \n",
       "the  0.344740 -0.243280  0.131390 -0.008877  0.336170  0.030591  0.255770  \n",
       ",   -0.146790  0.046971  0.032325 -0.220060 -0.207740 -0.231890 -0.108140  \n",
       ".    0.007003 -0.033006 -0.080021 -0.244510 -0.039174 -0.162360 -0.096652  \n",
       "of   0.233720  0.096027 -0.303240  0.244880 -0.086254 -0.419170  0.464960  \n",
       "to   0.131940 -0.332780  0.188770 -0.234220  0.544180 -0.230690  0.349470  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, you can view the embedding of a specific word using the code\n",
    "\n",
    "``embeddings_df.loc[word]``\n",
    "\n",
    "Try it with a few words of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '______' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6c1a2602e52b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselected_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m______\u001b[0m \u001b[1;31m# Fill in here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0membeddings_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselected_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '______' is not defined"
     ]
    }
   ],
   "source": [
    "selected_word = ______ # TODO: FILL IN HERE.\n",
    "embeddings_df.loc[selected_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a function to compute cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write a function to compute the cosine similarity between two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a: Fill in the function definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following function; feel free to discuss with your neighbors and look back to today's slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(word_a, word_b):\n",
    "    word_a_vector = embeddings_df.loc[word_a]\n",
    "    word_b_vector = _____ # TODO: FILL IN HERE\n",
    "    word_a_vectornorm = np.linalg.norm(word_a_vector)\n",
    "    word_b_vectornorm = ____ # TODO: FILL IN HERE\n",
    "    a_dot_b = np.dot(word_a_vector, ____ ) # TODO: FILL IN HERE\n",
    "    cosine_similarity = # TODO: FILL IN HERE\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b: Test your cosine similarity function with different words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out your function with some sample words, and write some examples of cosine similarities between words you tried. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_a = _____ # TODO: FILL IN HERE\n",
    "word_b = _____ # TODO: FILL IN HERE\n",
    "similarity = compute_cosine_similarity(word_a, word_b)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a function to find the closest words to a selected word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've written a function to compute the cosine similarity between two words, it's time to try solving our own analogies!\n",
    "\n",
    "As a recap, we can solve analogies using the following translation from words to math:\n",
    "\n",
    "$A\\:is\\:to\\:B\\:as\\:C\\:is\\:to\\:D \\leftrightarrow A-B\\approx C-D \\leftrightarrow C+B-A\\approx D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, try filling in the following function to solve an analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predicted_index_vectorized(v1, v2, v3, embeddings_matrix):\n",
    "    embeddings_index = embeddings_df.index\n",
    "    v1_index = embeddings_index.get_loc(v1)\n",
    "    v2_index = embeddings_index.get_loc(v2)\n",
    "    v3_index = embeddings_index.get_loc(v3)\n",
    "    v1 = embeddings_matrix[v1_index,:]\n",
    "    v2 = embeddings_matrix[v2_index,:]\n",
    "    v3 = embeddings_matrix[v3_index,:]\n",
    "    predicted_vec = (____ + ____ - ____).reshape(1, embeddings_df.shape[1]) # TODO: FILL IN HERE.\n",
    "    diffs = np.sum((embeddings_matrix - predicted_vec) ** 2, axis=1)\n",
    "    min_indices = diffs.argsort()[:4]\n",
    "    for i in range(3):\n",
    "        min_index = min_indices[i]\n",
    "        if min_index != v1_index and min_index != v2_index and min_index != v3_index:\n",
    "            return(embeddings_index[min_index])\n",
    "    return embeddings_index[min_indices[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Try solving your own analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using your function to solve your own analogies.\n",
    "\n",
    "To start, here are some you might try:\n",
    "\n",
    "    \"boy\" is to \"girl\" as \"brother\" is to ?\n",
    "    \"uncle\" is to \"aunt\" as \"policewoman\" is to ?\n",
    "    \"occasional\" is to \"occasionally\" as \"lucky\" is to ?\n",
    "    \"jumping\" is to \"jumped\" as \"flying\" is to ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: POSSIBLE OTHER STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your Own Word Vectors\n",
    "\n",
    "Now let's try creating our own word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create vectors using co-occurrence counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create vectors using PPMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize projections of these embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word vectors\n",
    "\n",
    "- Distribution of nearby words, with diff window sizes\n",
    "- PPMI\n",
    "- Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize projections of selected words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings are useful mathematically, but they have a lot of dimensions, which makes it hard to visualize them.\n",
    "\n",
    "For easier visualization, we can project them onto fewer dimensions. Using projection, we can map something in many dimensions (in the case of these word vectors, 200 dimensions) into fewer dimensions (in particular, two dimensions are easy to visualize).\n",
    "\n",
    "Projecting onto fewer dimensions makes us lose some information. For instance, imagine that we have a full-color picture and map it to a black-and-white picture -- the new picture might be easier to represent, but we lose some information in the process.\n",
    "\n",
    "Using certain techniques, we can make a lower-dimensional projection that keeps as much information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = X.mean(0)\n",
    "C = np.cov(X - mu, rowvar=False)\n",
    "d, u = np.linalg.eigh(C)\n",
    "U = u.T[::-1]\n",
    "Z = np.dot(X - mu, U[:2].T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
